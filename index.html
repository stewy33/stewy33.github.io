---
layout: default
---

<body>
<img src="{{ "/assets/images/stewy.jpg" | relative_url }}" height="320" class="circular" />

    <br>
    <p class="lead">
        I'm a third-year PhD student at MIT CSAIL, advised by
        <a href="https://people.csail.mit.edu/dhm/">Dylan Hadfield-Menell</a>.
        I develop tools to ensure increasingly powerful LLMs and AI agents are safe and aligned with human values. My work spans two main areas:
        <ol>
            <li>Adversarial defenses and evaluations</li>
            <li>Preference learning and alignment (<a href="https://arxiv.org/abs/2307.15217">[1]</a>)</li>
        </ol>
        I like building methods that leverage clear theoretical insights on important problems. My goal is to develop these insights into empirically effective, practical, and scalable tools for alignment and safety.<br/><br/>
        
        Previously, I obtained my bachelor's degree at Johns Hopkins, where I worked with Professor
        <a href="http://vision.jhu.edu/rvidal.html">Rene Vidal</a> on the theory of deep learning.
        During undergrad, I also worked with <a href="https://www.dynotx.com/">Dyno Therapeutics</a> and NASA on ML-guided protein
        design.<br/><br/>

        Outside of work, I enjoy playing piano, backpacking, salsa dancing, and philosophy.

        <br /><br/>
        Please reach out at <a href="">stew@csail.mit.edu</a> if you'd like to talk!
    </p>

    <br />
    <center>
        <a href="https://scholar.google.com/citations?user=Z9voXDgAAAAJ&hl" steyl="margin: 0.75em">Google Scholar</a>
        <a href="{{ "/assets/data/resume.pdf" | relative_url}}" style="margin: 0.75em">Resume</a>
        <a href="https://www.goodreads.com/review/list/99908226?shelf=read" style="margin: 0.75em">Goodreads</a>
        <!--<a href="https://github.com/stewy33" style="margin: 0.75em">Github</a>-->
    </center>
</body>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-08-13T14:03:25-04:00</updated><id>/feed.xml</id><title type="html">Stewy Slocum</title><subtitle>Stewy Slocum's website</subtitle><author><name>Stewart Slocum</name></author><entry><title type="html">Hutchinson’s Diagonal Estimator</title><link href="/2021/08/07/hutchinson's-diagonal-estimator.html" rel="alternate" type="text/html" title="Hutchinson’s Diagonal Estimator" /><published>2021-08-07T00:00:00-04:00</published><updated>2021-08-07T00:00:00-04:00</updated><id>/2021/08/07/hutchinson's-diagonal-estimator</id><content type="html" xml:base="/2021/08/07/hutchinson's-diagonal-estimator.html">&lt;blockquote&gt;
  &lt;p&gt;In this post, we’ll take a look at a method of approximating large Hessian matrices using a stochastic diagonal estimator. Hutchinson’s method can be used for optimization and loss-landscape analysis in deep neural networks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;hutchinsons-diagonal-estimator&quot;&gt;Hutchinson’s Diagonal Estimator&lt;/h1&gt;

&lt;p&gt;In modern machine learning with large deep models, explicit computation of the Hessian matrix is intractable. However, the Hessian matrix provides valuable information for optimization, studying generalization, and for other purposes. But even if we can’t calculate the full Hessian, can we effectively approximate it?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flat-minimum.png&quot; alt=&quot;Neural Network Loss Surfaces&quot; /&gt; &lt;em&gt;Fig. 1. Flat minima have been linked to improved generalization. The magnitude of the eigenvalues of the Hessian provide one way to characterize sharpness/flatness &lt;a class=&quot;citation&quot; href=&quot;#Keskar2017OnLT&quot;&gt;(Keskar et al., 2017)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There has been significant work towards efficient approximation of the Hessian, most notably in the form of low-rank updates like &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. This approach work well in traditional optimization settings, but is relatively slow and generally doesn’t work in stochastic settings &lt;a class=&quot;citation&quot; href=&quot;#bollapragada2018progressive&quot;&gt;(Bollapragada et al., 2018)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, there has also been a variety of work old and new to estimate the diagonal of the Hessian as a way of approximating the matrix as a whole, for pruning &lt;a class=&quot;citation&quot; href=&quot;#LeCun1989OptimalBD&quot;&gt;(LeCun et al., 1989)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Hassibi1992SecondOD&quot;&gt;(Hassibi &amp;amp; Stork, 1992)&lt;/a&gt; analyzing the loss landscape &lt;a class=&quot;citation&quot; href=&quot;#Yao2020PyHessianNN&quot;&gt;(Yao et al., 2020)&lt;/a&gt;, and optimization &lt;a class=&quot;citation&quot; href=&quot;#Yao2021ADAHESSIANAA&quot;&gt;(Yao et al., 2021)&lt;/a&gt;. It has been argued that the diagonal is a good approximation to the Hessian for machine learning problems, and that diagonal elements tend to be much larger than off-diagonal elements.&lt;/p&gt;

&lt;p&gt;However, calculating the diagonal of the Hessian is not straightforward. Automatic differentiation libraries are designed to make large computations in parallel, so naively calculating diagonal terms of the Hessian one-by-one \(\frac{\partial^2 L(x)}{\partial x_1^2}, \frac{\partial^2 L(x)}{\partial x_2^2}, ..., \frac{\partial^2 L(x)}{\partial x_n^2}\) requires \(n\) backprop operations and isn’t computationally feasible. However, we can estimate this diagonal relatively efficiently using randomized linear algebra in the form of Hutchinson’s estimator.&lt;/p&gt;

&lt;h2 id=&quot;hessian-vector-products&quot;&gt;Hessian-Vector Products&lt;/h2&gt;

&lt;p&gt;While calculating the Hessian as a whole isn’t possible, we &lt;em&gt;can&lt;/em&gt; efficiently estimate Hessian-vector products. There are a variety of ways to do this, the simplest being a finite difference approximation:&lt;/p&gt;

&lt;h3 id=&quot;1-finite-difference-approximation&quot;&gt;1. Finite Difference Approximation&lt;/h3&gt;

\[H(x) v \approx \frac{g(x + rv) - g(x - rv)}{2r}\]

&lt;p&gt;The error of this approximation is \(O ( r)\), which means it can be quite accurate when \(r\) is small. However, as \(r\) becomes small, rounding errors pile up in the numerator. While not accurate enough for every application, the finite difference approximation is simple and cheap (2 gradient evaluations).&lt;/p&gt;

&lt;h3 id=&quot;2-product-rule-trick&quot;&gt;2. Product Rule Trick&lt;/h3&gt;

\[\frac{\partial (g(x)^\top v)}{\partial x} = \frac{\partial g(x)}{\partial x} v + g(\theta) \frac{\partial v}{\partial x} = \frac{\partial g(x)}{\partial x} v = H(x) v\]

&lt;p&gt;This exact method requires taking the gradient of an expression that includes the gradient. It means that the operations used to backprop must also be tracked so they can be differentiated.&lt;/p&gt;

&lt;h3 id=&quot;3-perlmutters-trick-chain-rule-trick&quot;&gt;3. Perlmutter’s Trick (Chain Rule Trick)&lt;/h3&gt;

\[\frac{\partial g(x + r v)}{\partial r}|_{r=0} = H(x)v\]

&lt;p&gt;This method is described in more detail in &lt;a class=&quot;citation&quot; href=&quot;#Pearlmutter1994FastEM&quot;&gt;(Pearlmutter, 1994)&lt;/a&gt;. Like the product rule trick, it is exact, and requires differentiating and expression that includes the gradient.&lt;/p&gt;

&lt;p&gt;Each method has advantages and disadvantages, but the product rule trick is the implementation you’ll find for Hessian-vector products in &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp&quot;&gt;pytorch&lt;/a&gt; and in &lt;a href=&quot;https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html&quot;&gt;jax&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hutchinsons-estimator&quot;&gt;Hutchinson’s Estimator&lt;/h2&gt;

&lt;p&gt;Now that we have efficient Hessian-vector multiplication, we can use Hutchinson’s estimator.&lt;/p&gt;

&lt;h3 id=&quot;hutchinsons-trace-estimator&quot;&gt;Hutchinson’s Trace Estimator&lt;/h3&gt;

&lt;p&gt;Hutchinson’s original estimator is for the trace of a matrix. This estimator is more common than the diagonal estimator and has been more thoroughly analyzed, so let’s warm up by taking a look at it.&lt;/p&gt;

&lt;p&gt;To estimate the trace, we draw random vectors \(z\) from a distribution with mean zero and variance one (typically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rademacher_distribution&quot;&gt;Rademacher distribution&lt;/a&gt;). We can then estimate the trace of a matrix as \(\mathbb{E}[z^\top H z] = \text{trace}(H)\).&lt;/p&gt;

&lt;p&gt;Claim: Let \(z\) a random vector with \(\mathbb{E}[z]=0\), \(\mathbb{V}[z]=1\), and independent entries. Then \(\mathbb{E}[z^\top H z] = \text{trace}(H)\).&lt;/p&gt;

&lt;p&gt;Proof:&lt;/p&gt;

\[\begin{align}
	\mathbb{E}[z^\top H z] &amp;amp;= \mathbb{E}[\begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix}^\top \begin{bmatrix}H_{11} z_1 + H_{12} z_2 + ... + H_{1n} z_n \\ H_{12} z_1 + H_{22} z_2 + ... + H_{2n} z_n \\ ... \\ H_{n1} z_1 + H_{n2} z_2 + ... + H_{nn} z_n\end{bmatrix}] \\
	&amp;amp;= \mathbb{E}[\begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix}^\top \begin{bmatrix}\sum\limits_{i=1}^n H_{1i} z_i \\ \sum\limits_{i=1}^n H_{2i} z_i \\ ... \\ \sum\limits_{i=1}^n H_{ni} z_i\end{bmatrix}] \\
	&amp;amp;= \mathbb{E}[z_1 \sum\limits_{i=1}^n H_{1i} z_i + z_2 \sum\limits_{i=1}^n H_{2i} z_i + ... + z_n \sum\limits_{i=0}^n H_{ni} z_i] \\
	&amp;amp;= \sum\limits_{i=1}^n H_{1i} \mathbb{E}[z_1 z_i] + \sum\limits_{i=1}^n H_{2i} \mathbb{E}[z_2 z_i] + ... + \sum\limits_{i=1}^n H_{ni} \mathbb{E}[z_n z_i] \\
\end{align}\]

&lt;p&gt;And since the entries of \(z\) are independent, \(\mathbb{E}[z_i z_j] = \mathbb{E}[z_i]\mathbb{E}[z_j] = 0 \cdot 0 = 0\) for \(i \neq j\).&lt;/p&gt;

&lt;p&gt;However, when \(i = j\), then \(\mathbb{E}[z_i z_j] = \mathbb{E}[z_i^2] = \mathbb{V}[z_i] + \mathbb{E}[z_i]^2 = 1 + 0^2 = 1\).&lt;/p&gt;

&lt;p&gt;So&lt;/p&gt;

\[\begin{align}
  \mathbb{E}[z^\top H z] &amp;amp;= \sum\limits_{i=1}^n H_{1i} \mathbb{E}[z_1 z_i] + \sum\limits_{i=1}^n H_{2i} \mathbb{E}[z_2 z_i] + ... + \sum\limits_{i=1}^n H_{ni} \mathbb{E}[z_n z_i] \\
  &amp;amp;= H_{11} + H_{22} + ... + H_{nn} \\
  &amp;amp;= \text{trace}(H)
\end{align}\]

&lt;h3 id=&quot;hutchinsons-diagonal-estimator-1&quot;&gt;Hutchinson’s Diagonal Estimator&lt;/h3&gt;

&lt;p&gt;The basic idea from the trace estimator can be modified to give an estimator for the diagonal rather than the trace.&lt;/p&gt;

&lt;p&gt;Claim: Let \(z\) be a random variable \(z\) with \(\mathbb{E}[z]=0\), \(\mathbb{V}[z]=1\), and independent entries. Then \(\mathbb{E}[z \odot H z] = \text{diag}(H)\).&lt;/p&gt;

&lt;p&gt;Proof:&lt;/p&gt;

\[\begin{align*}
    \mathbb{E}[z \odot Hz] &amp;amp;= \mathbb{E}[
    \begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix} \odot
    \begin{bmatrix}
    H_{11} z_1 + H_{12} z_2 + ... + H_{1n} z_n \\
    H_{21} z_1 + H_{22} z_2 + ... + H_{2n} z_n \\ ... \\
    H_{n1} z_1 + H_{n2} z_2 + ... + H_{nn} z_n
    \end{bmatrix}
    ] \\
    &amp;amp;= \begin{bmatrix}
    H_{11} \mathbb{E}[z_1^2] + H_{12} \mathbb{E}[z_1z_2] + ... + H_{1n}[z_1z_n] \\
    H_{21} \mathbb{E}[z_2z_1] + H_{22} \mathbb{E}[z_2^2] + ... + H_{2n}[z_2z_n] \\ ... \\
    H_{n1} \mathbb{E}[z_nz_1] + H_{n2} \mathbb{E}[z_nz_2] + ... + H_{nn}[z_n^2]
    \end{bmatrix} \\
    &amp;amp;= \begin{bmatrix}
    H_{11} \\ H_{22} \\ ... \\ H_{nn}
    \end{bmatrix} \\
    &amp;amp;= \text{diag}(H)
\end{align*}\]

&lt;p&gt;Now let’s calculate the variance of Hutchinson’s diagonal estimator.&lt;/p&gt;

&lt;p&gt;Claim: Let \(z \sim \text{Rademacher}\). Then&lt;/p&gt;

\[\mathbb{V}[z \odot H z] = \begin{bmatrix}||H_1||^2 \\ ||H_2||^2 \\ ... \\ ||H_n||^2\end{bmatrix} - \text{diag}(H)^2\]

&lt;p&gt;where \(H_k\) is the k-th row of \(H\).&lt;/p&gt;

&lt;p&gt;Proof: Let us consider each entry \(k\) separately:&lt;/p&gt;

\[\begin{align}
    (\mathbb{V}[z \odot H z])_k &amp;amp;= \mathbb{V}[z_k \odot (H z)_k] \\
    &amp;amp;= \mathbb{V}[z_k (H_{k1} z_1 + ... + H_{kk} z_k + ... + H_{kn} z_n)] \\
    &amp;amp;= \mathbb{V}[H_{k1} z_k z_1 + ... + H_{kk} z_k z_k + ... + H_{kn} z_k z_n] \\
    &amp;amp;= \sum\limits_{i=0}^n \sum\limits_{j=0}^n \text{Cov}(H_{ki} z_k z_i, H_{kj} z_k z_j) \\
    &amp;amp;= \sum\limits_{i=0}^n \sum\limits_{j=0}^n \mathbb{E}[(H_{ki} z_k z_i)(H_{kj} z_k z_j)] - \mathbb{E}[H_{ki} z_k z_i] \mathbb{E}[H_{kj} z_k z_j] \\
    &amp;amp;= \sum\limits_{i=0}^n \sum\limits_{j=0}^n H_{ki}H_{kj} (\mathbb{E}[z_k^2 z_i z_)] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j]) \\
\end{align}\]

&lt;p&gt;When \(i \neq j\), we also know that at least one of \(i, j \neq k\). WLOG suppose \(i \neq k\). Then
\(\mathbb{E}[z_k^2 z_i z_j] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j] = \mathbb{E}[z_k^2 z_j]\mathbb{E}[z_i] - \mathbb{E}[z_k]\mathbb{E}[z_i]\mathbb{E}[z_k z_j] = 0 - 0 = 0\).&lt;/p&gt;

&lt;p&gt;Now since all terms of the sum with \(i \neq j\) are zero,&lt;/p&gt;

\[\begin{align}
	(\mathbb{V}[z \odot H z])_k &amp;amp;= \sum\limits_{i=0}^n H_{ki} H_{ki} (\mathbb{E}[z_k^2 z_i z_)] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j])
\end{align}\]

&lt;p&gt;And when \(i = j = k\), we have \(\mathbb{E}[z_k^2 z_i z_j] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j] = \mathbb{E}[z_k^4] - \mathbb{E}[z_k^2]^2 = 1 - 1^2 = 0\).&lt;/p&gt;

&lt;p&gt;We have \(\mathbb{E}[z_k^4] = 1\) because for the Rademacher distribution, \(\mathbb{E}[z^4] = \text{Kurtosis}[z] = 1\). In fact, the Bernoulli distribution has the smallest kurtosis of any distribution (kurtosis 1), and since the Rademacher distribution is just a scaled Bernoulli, it is the optimal distribution from which to draw \(z\) in terms of reducing variance of this estimator.&lt;/p&gt;

&lt;p&gt;So since the terms of the sum when \(i = j = k\) are zero, we have&lt;/p&gt;

\[\begin{align}
	(\mathbb{V}[z \odot H z])_k &amp;amp;= \sum\limits_{i=0, i \neq k}^n H_{ki} H_{ki} (\mathbb{E}[z_k^2 z_i z_)] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j])
\end{align}\]

&lt;p&gt;Finally, when \(i = j \neq k\), \(\mathbb{E}[z_k^2 z_i z_j] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j] = \mathbb{E}[z_k^2 z_i^2] - \mathbb{E}[z_k z_i]^2 = \mathbb{E}[z_k^2]\mathbb{E}[z_i^2] - (\mathbb{E}[z_k]\mathbb{E}[z_i])^2 = 1 \cdot 1 - 0^2 = 1\)&lt;/p&gt;

&lt;p&gt;So, we have&lt;/p&gt;

\[\begin{align}
  (\mathbb{V}[z \odot H z])_k &amp;amp;= \sum\limits_{i=0}^n \sum\limits_{j=0}^n H_{ki}H_{kj} (\mathbb{E}[z_k^2 z_i z_)] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j]) \\
    &amp;amp;= \sum\limits_{i=0, i \neq k}^n H_{ki}H_{ki} (\mathbb{E}[z_k^2 z_i z_)] - \mathbb{E}[z_k z_i] \mathbb{E}[z_k z_j]) \\
    &amp;amp;= \sum\limits_{i=0, i \neq k}^n H_{ki}^2 \\
    &amp;amp;= ||H_k||^2 - H_{kk}^2
\end{align}\]

&lt;p&gt;Putting this all together, we get the full variance&lt;/p&gt;

\[\begin{align}
    \mathbb{V}[z \odot H z] &amp;amp;= \begin{bmatrix}||H_1||^2 \\ ||H_2||^2 \\ ... \\ ||H_n||^2\end{bmatrix} - \text{diag}(H)^2
\end{align}\]

&lt;p&gt;Note that the entries of vector of variances are proportional to&lt;/p&gt;

\[||H_k^2||\]

&lt;p&gt;which grows linearly with the number of variables in the Hessian.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Keskar2017OnLT&quot;&gt;Keskar, N., Mudigere, D., Nocedal, J., Smelyanskiy, M., &amp;amp; Tang, P. T. P. (2017). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. &lt;i&gt;ArXiv&lt;/i&gt;, &lt;i&gt;abs/1609.04836&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bollapragada2018progressive&quot;&gt;Bollapragada, R., Nocedal, J., Mudigere, D., Shi, H.-J., &amp;amp; Tang, P. T. P. (2018). A progressive batching L-BFGS method for machine learning. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 620–629. https://arxiv.org/abs/1802.05374&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;LeCun1989OptimalBD&quot;&gt;LeCun, Y., Denker, J., &amp;amp; Solla, S. (1989). Optimal Brain Damage. &lt;i&gt;NIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hassibi1992SecondOD&quot;&gt;Hassibi, B., &amp;amp; Stork, D. (1992). Second Order Derivatives for Network Pruning: Optimal Brain Surgeon. &lt;i&gt;NIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yao2020PyHessianNN&quot;&gt;Yao, Z., Gholami, A., Keutzer, K., &amp;amp; Mahoney, M. W. (2020). PyHessian: Neural Networks Through the Lens of the Hessian. &lt;i&gt;2020 IEEE International Conference on Big Data (Big Data)&lt;/i&gt;, 581–590.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yao2021ADAHESSIANAA&quot;&gt;Yao, Z., Gholami, A., Shen, S., Keutzer, K., &amp;amp; Mahoney, M. (2021). ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning. &lt;i&gt;ArXiv&lt;/i&gt;, &lt;i&gt;abs/2006.00719&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pearlmutter1994FastEM&quot;&gt;Pearlmutter, B. A. (1994). Fast Exact Multiplication by the Hessian. &lt;i&gt;Neural Computation&lt;/i&gt;, &lt;i&gt;6&lt;/i&gt;, 147–160.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Stewart Slocum</name></author><category term="optimization" /><summary type="html">In this post, we’ll take a look at a method of approximating large Hessian matrices using a stochastic diagonal estimator. Hutchinson’s method can be used for optimization and loss-landscape analysis in deep neural networks.</summary></entry></feed>
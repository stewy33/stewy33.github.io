<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-12-25T22:18:37-05:00</updated><id>/feed.xml</id><title type="html">Stewy Slocum</title><subtitle>Stewy Slocum's website</subtitle><author><name>Stewart Slocum</name></author><entry><title type="html">Readings</title><link href="/2021/12/20/readings.html" rel="alternate" type="text/html" title="Readings" /><published>2021-12-20T00:00:00-05:00</published><updated>2021-12-20T00:00:00-05:00</updated><id>/2021/12/20/readings</id><content type="html" xml:base="/2021/12/20/readings.html">&lt;blockquote&gt;
  &lt;p&gt;List of things I’ve been reading and dates of “completion”. Only adding things that I’ve read thoroughly and understand beyond a superficial level.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;h3 id=&quot;12252021&quot;&gt;12/25/2021&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions &lt;a href=&quot;https://arxiv.org/abs/1902.08825&quot;&gt;[link]&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12232021&quot;&gt;12/23/2021&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Tensor Algebra and Tensor Analysis for Engineers, Chapter 1: Vectors and Tensors in a Finite-Dimensional Space &lt;a href=&quot;https://link.springer.com/content/pdf/10.1007%2F978-3-319-16342-0.pdf&quot;&gt;[link]&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12222021&quot;&gt;12/22/2021&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dual Space Preconditioning for Gradient Descent &lt;a href=&quot;https://arxiv.org/abs/1902.02257v4&quot;&gt;[link]&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12202021&quot;&gt;12/20/2021&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Monte Carlo Statistical Methods, Chapter 6: Markov Chains &lt;a href=&quot;https://mcube.lab.nycu.edu.tw/~cfung/docs/books/robert2004monte_carlo_statistical_methods.pdf&quot;&gt;[link]&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Stewart Slocum</name></author><summary type="html">List of things I’ve been reading and dates of “completion”. Only adding things that I’ve read thoroughly and understand beyond a superficial level.</summary></entry><entry><title type="html">Hutchinson’s Diagonal Estimator</title><link href="/2021/08/07/hutchinson's-diagonal-estimator.html" rel="alternate" type="text/html" title="Hutchinson’s Diagonal Estimator" /><published>2021-08-07T00:00:00-04:00</published><updated>2021-08-07T00:00:00-04:00</updated><id>/2021/08/07/hutchinson's-diagonal-estimator</id><content type="html" xml:base="/2021/08/07/hutchinson's-diagonal-estimator.html">&lt;blockquote&gt;
  &lt;p&gt;In this post, we’ll take a look at a method of approximating large Hessian matrices using a stochastic diagonal estimator. Hutchinson’s method can be used for optimization and loss-landscape analysis in deep neural networks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;In modern machine learning with large deep models, explicit computation of the Hessian matrix is intractable. However, the Hessian matrix provides valuable information for optimization, studying generalization, and for other purposes. But even if we can’t calculate the full Hessian, can we effectively approximate it?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flat-minimum.png&quot; alt=&quot;Neural Network Loss Surfaces&quot; /&gt; &lt;em&gt;Fig. 1. Flat minima have been linked to improved generalization. The magnitude of the eigenvalues of the Hessian provide one way to characterize sharpness/flatness &lt;a class=&quot;citation&quot; href=&quot;#Keskar2017OnLT&quot;&gt;(Keskar et al., 2017)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There has been significant work towards efficient approximation of the Hessian, most notably in the form of low-rank updates like &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. This approach work well in traditional optimization settings, but is relatively slow and generally doesn’t work in stochastic settings &lt;a class=&quot;citation&quot; href=&quot;#bollapragada2018progressive&quot;&gt;(Bollapragada et al., 2018)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, there has also been a variety of work old and new to estimate the diagonal of the Hessian as a way of approximating the matrix as a whole, for pruning &lt;a class=&quot;citation&quot; href=&quot;#LeCun1989OptimalBD&quot;&gt;(LeCun et al., 1989)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Hassibi1992SecondOD&quot;&gt;(Hassibi &amp;amp; Stork, 1992)&lt;/a&gt; analyzing the loss landscape &lt;a class=&quot;citation&quot; href=&quot;#Yao2020PyHessianNN&quot;&gt;(Yao et al., 2020)&lt;/a&gt;, and optimization &lt;a class=&quot;citation&quot; href=&quot;#Yao2021ADAHESSIANAA&quot;&gt;(Yao et al., 2021)&lt;/a&gt;. It has been argued that the diagonal is a good approximation to the Hessian for machine learning problems, and that diagonal elements tend to be much larger than off-diagonal elements.&lt;/p&gt;

&lt;p&gt;However, calculating the diagonal of the Hessian is not straightforward. Automatic differentiation libraries are designed to make large computations in parallel, so naively calculating diagonal terms of the Hessian one-by-one \(\frac{\partial^2 L(x)}{\partial x_1^2}, \frac{\partial^2 L(x)}{\partial x_2^2}, ..., \frac{\partial^2 L(x)}{\partial x_n^2}\) requires \(n\) backprop operations and isn’t computationally feasible. However, we can estimate this diagonal relatively efficiently using randomized linear algebra in the form of Hutchinson’s estimator.&lt;/p&gt;

&lt;h2 id=&quot;hessian-vector-products&quot;&gt;Hessian-Vector Products&lt;/h2&gt;

&lt;p&gt;While calculating the Hessian as a whole isn’t possible, we &lt;em&gt;can&lt;/em&gt; efficiently estimate Hessian-vector products. There are a variety of ways to do this, the simplest being a finite difference approximation:&lt;/p&gt;

&lt;h3 id=&quot;1-finite-difference-approximation&quot;&gt;1. Finite Difference Approximation&lt;/h3&gt;

\[H(x) v \approx \frac{g(x + rv) - g(x - rv)}{2r}\]

&lt;p&gt;The error of this approximation is \(O(r)\), which means it can be quite accurate when \(r\) is small. However, as \(r\) becomes small, rounding errors pile up in the numerator. While not accurate enough for every application, the finite difference approximation is simple and cheap (2 gradient evaluations).&lt;/p&gt;

&lt;h3 id=&quot;2-product-rule-trick&quot;&gt;2. Product Rule Trick&lt;/h3&gt;

\[\frac{\partial (g(x)^\top v)}{\partial x} = \frac{\partial g(x)}{\partial x} v + g(x) \frac{\partial v}{\partial x} = \frac{\partial g(x)}{\partial x} v = H(x) v\]

&lt;p&gt;This exact method requires taking the gradient of an expression that includes the gradient. It means that the operations used to backprop must also be tracked so they can be differentiated.&lt;/p&gt;

&lt;h3 id=&quot;3-perlmutters-trick-chain-rule-trick&quot;&gt;3. Perlmutter’s Trick (Chain Rule Trick)&lt;/h3&gt;

\[\frac{\partial g(x + r v)}{\partial r}|_{r=0} = H(x)v\]

&lt;p&gt;This method is described in more detail in &lt;a class=&quot;citation&quot; href=&quot;#Pearlmutter1994FastEM&quot;&gt;(Pearlmutter, 1994)&lt;/a&gt;. Like the product rule trick, it is exact, and requires differentiating and expression that includes the gradient.&lt;/p&gt;

&lt;p&gt;Each method has advantages and disadvantages, but the product rule trick is the implementation you’ll find for Hessian-vector products in &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp&quot;&gt;pytorch&lt;/a&gt; and in &lt;a href=&quot;https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html&quot;&gt;jax&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hutchinsons-estimator&quot;&gt;Hutchinson’s Estimator&lt;/h2&gt;

&lt;p&gt;Now that we have efficient Hessian-vector multiplication, we can use Hutchinson’s estimator.&lt;/p&gt;

&lt;h3 id=&quot;hutchinsons-trace-estimator&quot;&gt;Hutchinson’s Trace Estimator&lt;/h3&gt;

&lt;p&gt;Hutchinson’s original estimator is for the trace of a matrix. This estimator is more common than the diagonal estimator and has been more thoroughly analyzed, so let’s warm up by taking a look at it.&lt;/p&gt;

&lt;p&gt;To estimate the trace, we draw random vectors \(z\) from a distribution with mean zero and variance one (typically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rademacher_distribution&quot;&gt;Rademacher distribution&lt;/a&gt;). We can then estimate the trace of a matrix as \(\mathbb{E}[z^\top H z] = \text{trace}(H)\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Let \(z\) a random vector with \(\mathbb{E}[z]=0\), \(\mathbb{V}[z]=1\), and independent entries. Then \(\mathbb{E}[z^\top H z] = \text{trace}(H)\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;

\[\begin{align}
	\mathbb{E}[z^\top H z] &amp;amp;= \mathbb{E}[\begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix}^\top \begin{bmatrix}H_{11} z_1 + H_{12} z_2 + ... + H_{1n} z_n \\ H_{12} z_1 + H_{22} z_2 + ... + H_{2n} z_n \\ ... \\ H_{n1} z_1 + H_{n2} z_2 + ... + H_{nn} z_n\end{bmatrix}] \\
	&amp;amp;= \mathbb{E}[\begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix}^\top \begin{bmatrix}\sum\limits_{i=1}^n H_{1i} z_i \\ \sum\limits_{i=1}^n H_{2i} z_i \\ ... \\ \sum\limits_{i=1}^n H_{ni} z_i\end{bmatrix}] \\
	&amp;amp;= \mathbb{E}[z_1 \sum\limits_{i=1}^n H_{1i} z_i + z_2 \sum\limits_{i=1}^n H_{2i} z_i + ... + z_n \sum\limits_{i=0}^n H_{ni} z_i] \\
	&amp;amp;= \sum\limits_{i=1}^n H_{1i} \mathbb{E}[z_1 z_i] + \sum\limits_{i=1}^n H_{2i} \mathbb{E}[z_2 z_i] + ... + \sum\limits_{i=1}^n H_{ni} \mathbb{E}[z_n z_i] \\
\end{align}\]

&lt;p&gt;And since the entries of \(z\) are independent, \(\mathbb{E}[z_i z_j] = \mathbb{E}[z_i]\mathbb{E}[z_j] = 0 \cdot 0 = 0\) for \(i \neq j\).&lt;/p&gt;

&lt;p&gt;However, when \(i = j\), then \(\mathbb{E}[z_i z_j] = \mathbb{E}[z_i^2] = \mathbb{V}[z_i] + \mathbb{E}[z_i]^2 = 1 + 0^2 = 1\).&lt;/p&gt;

&lt;p&gt;So&lt;/p&gt;

\[\begin{align}
  \mathbb{E}[z^\top H z] &amp;amp;= \sum\limits_{i=1}^n H_{1i} \mathbb{E}[z_1 z_i] + \sum\limits_{i=1}^n H_{2i} \mathbb{E}[z_2 z_i] + ... + \sum\limits_{i=1}^n H_{ni} \mathbb{E}[z_n z_i] \\
  &amp;amp;= H_{11} + H_{22} + ... + H_{nn} \\
  &amp;amp;= \text{trace}(H) \tag*{$\blacksquare$}
\end{align}\]

&lt;h3 id=&quot;hutchinsons-diagonal-estimator&quot;&gt;Hutchinson’s Diagonal Estimator&lt;/h3&gt;

&lt;p&gt;The basic idea from the trace estimator can be modified to give an estimator for the diagonal rather than the trace.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Let \(z\) be a random variable \(z\) with \(\mathbb{E}[z]=0\), \(\mathbb{V}[z]=1\), and independent entries. Then \(\mathbb{E}[z \odot H z] = \text{diag}(H)\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;

\[\begin{align*}
    \mathbb{E}[z \odot Hz] &amp;amp;= \mathbb{E}[
    \begin{bmatrix}z_1 \\ z_2 \\ ... \\ z_n\end{bmatrix} \odot
    \begin{bmatrix}
    H_{11} z_1 + H_{12} z_2 + ... + H_{1n} z_n \\
    H_{21} z_1 + H_{22} z_2 + ... + H_{2n} z_n \\ ... \\
    H_{n1} z_1 + H_{n2} z_2 + ... + H_{nn} z_n
    \end{bmatrix}
    ] \\
    &amp;amp;= \begin{bmatrix}
    H_{11} \mathbb{E}[z_1^2] + H_{12} \mathbb{E}[z_1z_2] + ... + H_{1n}[z_1z_n] \\
    H_{21} \mathbb{E}[z_2z_1] + H_{22} \mathbb{E}[z_2^2] + ... + H_{2n}[z_2z_n] \\ ... \\
    H_{n1} \mathbb{E}[z_nz_1] + H_{n2} \mathbb{E}[z_nz_2] + ... + H_{nn}[z_n^2]
    \end{bmatrix} \\
    &amp;amp;= \begin{bmatrix}
    H_{11} \\ H_{22} \\ ... \\ H_{nn}
    \end{bmatrix} \\
    &amp;amp;= \text{diag}(H) \tag*{$\blacksquare$}
\end{align*}\]

&lt;p&gt;Now let’s calculate the variance of Hutchinson’s diagonal estimator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Let \(z \sim \text{Rademacher}\). Then the covariance matrix of Hutchinson’s diagonal estimator is&lt;/p&gt;

\[\begin{align}
    \Sigma_{z \odot Hz} = \text{diag}(H)\text{diag}(H)^\top + \left(\begin{bmatrix}
    ||H_1||^2 \\
    ||H_2||^2 \\
    \vdots \\
    ||H_n||^2
    \end{bmatrix} - 2 \text{diag}(H)^2\right) \odot I
\end{align}\]

&lt;p&gt;where \(H_i\) is the \(i\)-th row of \(H\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Let us consider each entry of the covariance matrix separately:&lt;/p&gt;

\[\begin{align}
    (\Sigma_{z \odot Hz})_{ij} &amp;amp;= \text{Cov}[(z \odot Hz)_i (z \odot Hz)_j] \\
    &amp;amp;= \mathbb{E}[(z \odot Hz)_i (z \odot Hz)_j] - \mathbb{E}[z \odot Hz]_i \mathbb{E}[z \odot Hz]_j \\
    &amp;amp;= \mathbb{E}[(z_i (H_{i1} z_1 + ... + H_{in} z_n)) (z_j (H_{j1} z_1 + ... + H_{jn} z_n))] - H_{ii} H_{jj} \\
    &amp;amp;= \mathbb{E}[\sum\limits_{k=0}^n \sum\limits_{l=0}^n H_{ik}H_{jl} z_i z_k z_j z_l] - H_{ii}H_{jj} \\
    &amp;amp;= (\sum\limits_{k=0}^n \sum\limits_{l=0}^n H_{ik} H_{jl} \mathbb{E}[z_i z_k z_j z_l]) - H_{ii}H_{jj}
\end{align}\]

&lt;p&gt;First consider diagonal elements, which have \(i = j\):&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Case 1. \(i = j = k = l\). Then \(\mathbb{E}[z_i z_k z_j z_l] = \mathbb{E}[z_i^4] = \text{Kurtosis}[z]\)&lt;/li&gt;
  &lt;li&gt;Case 2. \(i = j \neq k\), \(k = l\). Then \(\mathbb{E}[z_i z_k z_j z_l] = \mathbb{E}[z_i^2]\mathbb{E}[z_k^2] = 1 \cdot 1 = 1\)&lt;/li&gt;
  &lt;li&gt;Case 3. \(i = j\), \(k \neq l\). Therefore, at least one of \(k, l \neq i\). WLOG suppose \(k \neq i\). Then \(\mathbb{E}[z_i z_k z_j z_l] = \mathbb{E}[z_i^2 z_l]\mathbb{E}[z_k] = 0 \cdot \mathbb{E}[z_i^2 z_l] = 0\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives&lt;/p&gt;

\[\begin{align}
    (\Sigma_{z \odot Hz})_{ii} &amp;amp;= (\sum\limits_{k=0}^n \sum\limits_{l=0}^n H_{ik} H_{il} \mathbb{E}[z_i z_k z_i z_l]) - H_{ii}^2 \\
    &amp;amp;= \text{Kurtosis}[z]H_{ii}^2 +  \sum\limits_{k=0, k \neq i}^n H_{ik}^2 - H_{ii}^2 \\
    &amp;amp;= \sum\limits_{k=0, k \neq i}^n H_{ik}^2
\end{align}\]

&lt;p&gt;since the kurtosis of a Rademacher RV is 1. In fact, the Bernoulli distribution has the smallest kurtosis of any distribution at 1, and since the Rademacher distribution is just a scaled Bernoulli, it is the optimal distribution to draw \(z\) from with respect to the variance of our estimator.&lt;/p&gt;

&lt;p&gt;Now consider off-diagonal elements, which have \(i \neq j\):&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Case 1. \(i \neq j\) with \(i = k, j = l\) or \(i = l, j = k\). Then \(\mathbb{E}[z_i z_k z_j z_l] = \mathbb{E}[z_i^2]\mathbb{E}[z_j]^2 = 1 \cdot 1 = 1\)&lt;/li&gt;
  &lt;li&gt;Case 2. \(i \neq j\) with at least one of \(k, l\) not equal to \(i\) or \(j\). WLOG suppose \(k \neq i, j\). Then \(\mathbb{E}[z_i z_k z_j z_l] = \mathbb{E}[z_i z_j z_l]\mathbb{E}[z_k] = \mathbb{E}[z_i z_j z_l] \cdot 0 = 0\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives&lt;/p&gt;

\[\begin{align}
    (\Sigma_{z \odot Hz})_{ij} &amp;amp;= (\sum\limits_{k=0}^n \sum\limits_{l=0}^n H_{ik} H_{jl} \mathbb{E}[z_i z_k z_j z_l]) - H_{ii}H_{jj} \\
    &amp;amp;= 2H_{ii}H_{jj} - H_{ii}H_{jj} \\
    &amp;amp;= H_{ii}H_{jj}
\end{align}\]

&lt;p&gt;So our final covariance matrix is&lt;/p&gt;

\[\require{color}
\definecolor{brand}{RGB}{78, 182, 133}
\begin{align}
    \Sigma_{z \odot Hz} &amp;amp;=
    \begin{bmatrix}
        \textcolor{brand}{\sum\limits_{k=0, k \neq 1}^n H_{1k}^2} &amp;amp; H_{11}H_{22} &amp;amp; H_{11}H_{33} &amp;amp; ... &amp;amp; H_{11}H_{nn} \\
        H_{11}H_{22} &amp;amp; \textcolor{brand}{\sum\limits_{k=0, k \neq 2}^n H_{2k}^2} &amp;amp; H_{22}H_{33} &amp;amp; ... &amp;amp; H_{22}H_{nn} \\
        H_{11}H_{33} &amp;amp; H_{22}H_{33} &amp;amp; \textcolor{brand}{\sum\limits_{k=0, k \neq 3}^n H_{3k}^2} &amp;amp; ... &amp;amp; H_{33}H_{nn} \\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; H_{n-1n-1}H_{nn} \\
        H_{11}H_{nn} &amp;amp; H_{22}H_{nn} &amp;amp; H_{33}H_{nn} &amp;amp; ... &amp;amp; \textcolor{brand}{\sum\limits_{k=0, k \neq n}^n H_{nk}^2}
    \end{bmatrix} \\
    &amp;amp;= \underbrace{\text{diag}(H)\text{diag}(H)^\top - \text{diag}(H)^2 \odot I}_{\text{off-diagonal covariances}} + \underbrace{\textcolor{brand}{\left(\begin{bmatrix}
    ||H_1||^2 \\
    ||H_2||^2 \\
    \vdots \\
    ||H_n||^2
    \end{bmatrix} - \text{diag}(H)^2\right) \odot I}}_{\text{diagonal vector of variances}} \\
    &amp;amp;= \text{diag}(H)\text{diag}(H)^\top + \left(\begin{bmatrix}
    ||H_1||^2 \\
    ||H_2||^2 \\
    \vdots \\
    ||H_n||^2
    \end{bmatrix} - 2 \text{diag}(H)^2\right) \odot I \tag*{$\blacksquare$}
\end{align}\]

&lt;p&gt;Note that &lt;em&gt;variance&lt;/em&gt; of our estimator (diagonal of tbe covariance matrix) at a specific output \(i\) is \(\mathbb{V}[(z \odot H z)_i] = \textcolor{brand}{\sum\limits_{k=0, k \neq i}^n H_{ik}^2} = \| H_i \| ^2 - H_{ii}^2\). Interestingly, this variance is equal to the squared L-2 norm of the off-diagonal elements of each row. So even though this variance grows like \(O(n)\) with the number of variables in the Hessian, if the assumption that the off-diagonal elements of the Hessian are small is true, then this variance remains small.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
Now let’s take a break with this landscape by Rembrandt&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/the-mill.jpg&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;&lt;strong&gt;The Mill&lt;/strong&gt;&lt;/p&gt;

&lt;p class=&quot;center&quot;&gt;Courtesy National Gallery of Art, Washington&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Keskar2017OnLT&quot;&gt;Keskar, N., Mudigere, D., Nocedal, J., Smelyanskiy, M., &amp;amp; Tang, P. T. P. (2017). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. &lt;i&gt;ArXiv&lt;/i&gt;, &lt;i&gt;abs/1609.04836&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bollapragada2018progressive&quot;&gt;Bollapragada, R., Nocedal, J., Mudigere, D., Shi, H.-J., &amp;amp; Tang, P. T. P. (2018). A progressive batching L-BFGS method for machine learning. &lt;i&gt;International Conference on Machine Learning&lt;/i&gt;, 620–629. https://arxiv.org/abs/1802.05374&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;LeCun1989OptimalBD&quot;&gt;LeCun, Y., Denker, J., &amp;amp; Solla, S. (1989). Optimal Brain Damage. &lt;i&gt;NIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Hassibi1992SecondOD&quot;&gt;Hassibi, B., &amp;amp; Stork, D. (1992). Second Order Derivatives for Network Pruning: Optimal Brain Surgeon. &lt;i&gt;NIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yao2020PyHessianNN&quot;&gt;Yao, Z., Gholami, A., Keutzer, K., &amp;amp; Mahoney, M. W. (2020). PyHessian: Neural Networks Through the Lens of the Hessian. &lt;i&gt;2020 IEEE International Conference on Big Data (Big Data)&lt;/i&gt;, 581–590.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yao2021ADAHESSIANAA&quot;&gt;Yao, Z., Gholami, A., Shen, S., Keutzer, K., &amp;amp; Mahoney, M. (2021). ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning. &lt;i&gt;ArXiv&lt;/i&gt;, &lt;i&gt;abs/2006.00719&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pearlmutter1994FastEM&quot;&gt;Pearlmutter, B. A. (1994). Fast Exact Multiplication by the Hessian. &lt;i&gt;Neural Computation&lt;/i&gt;, &lt;i&gt;6&lt;/i&gt;, 147–160.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Stewart Slocum</name></author><category term="optimization" /><summary type="html">In this post, we’ll take a look at a method of approximating large Hessian matrices using a stochastic diagonal estimator. Hutchinson’s method can be used for optimization and loss-landscape analysis in deep neural networks.</summary></entry></feed>
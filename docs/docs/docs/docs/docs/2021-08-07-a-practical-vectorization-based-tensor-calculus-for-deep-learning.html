<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>A Practical Vectorization-based Tensor Calculus for Deep Learning</title>
    <meta name="title" property="og:title" content="A Practical Vectorization-based Tensor Calculus for Deep Learning">
    <meta name="description" property="og:description" content="Stewy Slocum&#39;s website">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/2021-08-07-a-practical-vectorization-based-tensor-calculus-for-deep-learning.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LJGHV07M45"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-LJGHV07M45');
    </script>

    <!-- For Facebook share button -->
    <div id="fb-root"></div>
    <script>
        (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.0";
        fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));
    </script>

</head>


  <body>

    <header class="site-header" role="banner">

    <div class="wrapper">
        
        <a class="site-title" href="/">
            <img src="/assets/images/S.png" height="50px" style="display: none; position: relative; top: -5px; right: -7px"/>
            Stewy Slocum
        </a>

        <nav class="site-nav">
            <a class="page-link" href="/">&#127968; Home</a>
            <a class="page-link" href="/writing.html">&#9997; Writing</a>
        </nav>

    </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">A Practical Vectorization-based Tensor Calculus for Deep Learning</h1>
    <p class="post-meta">

      <time datetime="2021-08-07T00:00:00-04:00" itemprop="datePublished">
        
        Aug 7, 2021
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Stewart Slocum</span>
      </span>

      <span>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2021-08-07-a-practical-vectorization-based-tensor-calculus-for-deep-learning.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>A tutorial on a systematic yet approachable method of calculating matrix and tensor derivatives with applications in machine learning.</p>
</blockquote>

<!--more-->
<p>\(\newcommand{\bm}[1]{\boldsymbol{#1}}\)
When introductory machine learning courses cover gradient backpropagation, students often find themselves caught up in a seemingly arbitrary mess of matrices and transposes without explanation. The multivariable versions of the product rule and chain rule are provided without clear instruction on the subleties of applying them.</p>

<p>Some resources avoid these subtleties by computing matrix derivatives using element-wise partial derivatives, but this is confusing to apply and results in a loss of the underlying matrix and tensor structure. And when working with higher-order derivatives like the Hessian, this approach is hardly feasible. Handbooks of identities for matrix calculus like the <a href="http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf">Matrix Cookbook</a> are of some value, but don’t teach a systematic approach, so readers have trouble moving on to more complex cases. This is all taught so poorly that most people either fearfully avoid analytical calculation of neural network derivatives or resort to guessing and checking dimensions.</p>

<p>While there are more advanced treatments of tensor calculus using <a href="https://en.wikipedia.org/wiki/Ricci_calculus">Ricci Calculus</a> (tensor index notation), here I will share an approach that I find more straightforward, based on matrix vectorization.</p>

<p>My goal is for this post to leave you with the necessary knowledge and machinery to confidently take derivatives of matrices and tensors. Later on, I will work through some examples, including calculating gradients and Hessians of deep neural networks. If this post is unclear or you find errors, please let me know!</p>

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#basic-concepts" id="markdown-toc-basic-concepts">Basic Concepts</a>    <ul>
      <li><a href="#layout-conventions" id="markdown-toc-layout-conventions">Layout Conventions</a></li>
      <li><a href="#the-vectorization-operator" id="markdown-toc-the-vectorization-operator">The Vectorization Operator</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="basic-concepts">Basic Concepts</h2>
<p>In general, for matrices \(\bm{Y} \in \mathbb{R}^{l \times m}, \bm{X} \in \mathbb{R}^{n \times o}\), the matrix-matrix derivative</p>

\[\frac{\partial \bm{Y}}{\partial \bm{X}} \in \mathbb{R}^{l \times m \times n \times o}\]

<p>is a fourth order tensor. Rather than work with tensors and their complex machinery directly, we will simplify our calculations by row-wise vectorizing input and output matrices, defining</p>

\[\frac{\partial \bm{Y}}{\partial \bm{X}} := \frac{\partial \text{vec}_r(\bm{Y})}{\partial \text{vec}_r(\bm{X})^\top} \in \mathbb{R}^{lm \times no}\]

<p>to get <em>matrices</em> out of matrix-matrix derivatives. This gives a systematic and approachable technique to doing tensor calculus that leverages familiar vectors and matrices.</p>

<h3 id="layout-conventions">Layout Conventions</h3>
<p>This is a short summary on layout conventions in matrix calculus. For more information, see <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">matrix calculus layout conventions</a> on Wikipedia.</p>

<p>Let \(\bm{y} \in \mathbb{R}^m, \bm{x} \in \mathbb{R}^n\). Confusingly, different authors use two different layout conventions for vector-vector derivatives \(\frac{\partial \bm{y}}{\partial \bm{x}}\)</p>
<ol>
  <li><strong>Numerator layout</strong> - The derivative \(\frac{\partial \bm{y}}{\partial \bm{x}}\) is laid out according to \(\bm{y}\) and \(\bm{x}^\top\). In this layout, \(\frac{\partial \bm{y}}{\partial \bm{x}} := \frac{\partial \bm{y}}{\partial \bm{x}^\top}\) is an \(m \times n\) matrix, like a standard Jacobian.</li>
  <li><strong>Denominator layout</strong> - The derivative \(\frac{\partial \bm{y}}{\partial \bm{x}}\) is laid out according to \(\bm{y}^\top\) and \(\bm{x}\). In this layout, \(\frac{\partial \bm{y}}{\partial \bm{x}} := \frac{\partial \bm{y}^\top}{\partial \bm{x}}\) is an \(n \times m\) matrix.</li>
</ol>

<p>In this article, I will stick to the more popular <em>numerator layout</em> and use the explicit notation \(\frac{\partial \bm{y}}{\partial \bm{x}^\top}\) to communicate that I am using the numerator layout. However, when reading other authors, you might find either notation, or sometimes even a mixture of notation explanation.</p>

<h3 id="the-vectorization-operator">The Vectorization Operator</h3>
<p>jklj
<br /><br /></p>

<hr />

<p><br />
Now let’s take a break with</p>

<h2 id="references">References</h2>

<ol class="bibliography"></ol>

  </div>


  <div class="page-navigation">
    

    
  </div>

  
    <div id="disqus_thread"></div>
<script>
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://stewyslocum.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;"/>
<footer class="site-footer">
    2021 &copy; by Stewart Slocum | Built with <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/jekyll/minima/" target="_blank">minima</a>
    | View on <a href="https://github.com/stewy33/stewy33.github.io/" target="_blank">Github</a>
    | Inspired by <a href="https://lilianweng.github.io/lil-log/", target="_blank">Lilian Weng</a>
</footer>


  </body>

</html>
